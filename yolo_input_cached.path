diff --git a/official/vision/beta/projects/yolo/dataloaders/yolo_detection_input.py b/official/vision/beta/projects/yolo/dataloaders/yolo_detection_input.py
index 01759c60..c41ee2a8 100644
--- a/official/vision/beta/projects/yolo/dataloaders/yolo_detection_input.py
+++ b/official/vision/beta/projects/yolo/dataloaders/yolo_detection_input.py
@@ -13,6 +13,7 @@ from official.vision.beta.projects.yolo.utils import box_ops as box_utils
 
 class Parser(parser.Parser):
   """Parser to parse an image and its annotations into a dictionary of tensors."""
+
   def __init__(
       self,
       image_w=416,
@@ -57,15 +58,15 @@ class Parser(parser.Parser):
     """
     self._net_down_scale = net_down_scale
     self._image_w = (image_w // self._net_down_scale) * self._net_down_scale
-    self._image_h = self.image_w if image_h == None else (
+    self._image_h = self.image_w if image_h is None else (
         image_h // self._net_down_scale) * self._net_down_scale
     self._max_process_size = max_process_size
     self._min_process_size = min_process_size
     self._anchors = anchors
 
     self._fixed_size = fixed_size
-    self._jitter_im = 0.0 if jitter_im == None else jitter_im
-    self._jitter_boxes = 0.0 if jitter_boxes == None else jitter_boxes
+    self._jitter_im = 0.0 if jitter_im is None else jitter_im
+    self._jitter_boxes = 0.0 if jitter_boxes is None else jitter_boxes
     self._pct_rand = pct_rand
     self._max_num_instances = max_num_instances
     self._random_flip = random_flip
@@ -80,23 +81,22 @@ class Parser(parser.Parser):
         labels: a dict of Tensors that contains labels.
     """
 
-    shape = tf.shape(data["image"])
-    image = data["image"] / 255
+    shape = tf.shape(data['image'])
+    image = data['image'] / 255
     image = tf.image.resize(image,
                             (self._max_process_size, self._max_process_size))
     image = tf.image.random_brightness(image=image, max_delta=.1)  # Brightness
-    image = tf.image.random_saturation(image=image, lower=0.75,
-                                       upper=1.25)  # Saturation
+    image = tf.image.random_saturation(
+        image=image, lower=0.75, upper=1.25)  # Saturation
     image = tf.image.random_hue(image=image, max_delta=.1)  # Hue
     image = tf.clip_by_value(image, 0.0, 1.0)
-    boxes = data["groundtruth_boxes"]
+    boxes = data['groundtruth_boxes']
     image_shape = tf.shape(image)[:2]
 
     self._random_flip = False
     if self._random_flip:
-      image, boxes, _ = preprocess_ops.random_horizontal_flip(image,
-                                                              boxes,
-                                                              seed=self._seed)
+      image, boxes, _ = preprocess_ops.random_horizontal_flip(
+          image, boxes, seed=self._seed)
 
     randscale = self._image_w // self._net_down_scale
 
@@ -133,34 +133,31 @@ class Parser(parser.Parser):
         target_width=randscale * self._net_down_scale,
         target_height=randscale * self._net_down_scale)
 
-    best_anchors = preprocessing_ops.get_best_anchor(boxes,
-                                                     self._anchors,
-                                                     width=self._image_w,
-                                                     height=self._image_h)
+    best_anchors = preprocessing_ops.get_best_anchor(
+        boxes, self._anchors, width=self._image_w, height=self._image_h)
 
-    #padding
+    # padding
     boxes = preprocess_ops.clip_or_pad_to_fixed_size(boxes,
-                                                     self._max_num_instances,
-                                                     0)
+                                                     self._max_num_instances, 0)
     classes = preprocess_ops.clip_or_pad_to_fixed_size(
-        data["groundtruth_classes"], self._max_num_instances, -1)
+        data['groundtruth_classes'], self._max_num_instances, -1)
     best_anchors = preprocess_ops.clip_or_pad_to_fixed_size(
         best_anchors, self._max_num_instances, 0)
-    area = preprocess_ops.clip_or_pad_to_fixed_size(data["groundtruth_area"],
+    area = preprocess_ops.clip_or_pad_to_fixed_size(data['groundtruth_area'],
                                                     self._max_num_instances, 0)
     is_crowd = preprocess_ops.clip_or_pad_to_fixed_size(
-        tf.cast(data["groundtruth_is_crowd"], tf.int32),
+        tf.cast(data['groundtruth_is_crowd'], tf.int32),
         self._max_num_instances, 0)
     labels = {
-        "source_id": data["source_id"],
-        "bbox": boxes,
-        "classes": classes,
-        "area": area,
-        "is_crowd": is_crowd,
-        "best_anchors": best_anchors,
-        "width": shape[1],
-        "height": shape[2],
-        "num_detections": tf.shape(data["groundtruth_classes"])[0],
+        'source_id': data['source_id'],
+        'bbox': boxes,
+        'classes': classes,
+        'area': area,
+        'is_crowd': is_crowd,
+        'best_anchors': best_anchors,
+        'width': shape[1],
+        'height': shape[2],
+        'num_detections': tf.shape(data['groundtruth_classes'])[0],
     }
     return image, labels
 
@@ -173,37 +170,34 @@ class Parser(parser.Parser):
       labels: a dict of Tensors that contains labels.
     """
 
-    shape = tf.shape(data["image"])
-    image = tf.convert_to_tensor(data["image"])
+    shape = tf.shape(data['image'])
+    image = tf.convert_to_tensor(data['image'])
     image = tf.image.resize(image, size=(self._image_w, self._image_h))
     image = image / 255
-    boxes = box_utils.yxyx_to_xcycwh(data["groundtruth_boxes"])
-    best_anchors = preprocessing_ops.get_best_anchor(boxes,
-                                                     self._anchors,
-                                                     width=self._image_w,
-                                                     height=self._image_h)
-    #padding
+    boxes = box_utils.yxyx_to_xcycwh(data['groundtruth_boxes'])
+    best_anchors = preprocessing_ops.get_best_anchor(
+        boxes, self._anchors, width=self._image_w, height=self._image_h)
+    # padding
     boxes = preprocess_ops.clip_or_pad_to_fixed_size(boxes,
-                                                     self._max_num_instances,
-                                                     0)
+                                                     self._max_num_instances, 0)
     classes = preprocess_ops.clip_or_pad_to_fixed_size(
-        data["groundtruth_classes"], self._max_num_instances, -1)
+        data['groundtruth_classes'], self._max_num_instances, -1)
     best_anchors = preprocess_ops.clip_or_pad_to_fixed_size(
         best_anchors, self._max_num_instances, 0)
-    area = preprocess_ops.clip_or_pad_to_fixed_size(data["groundtruth_area"],
+    area = preprocess_ops.clip_or_pad_to_fixed_size(data['groundtruth_area'],
                                                     self._max_num_instances, 0)
     is_crowd = preprocess_ops.clip_or_pad_to_fixed_size(
-        tf.cast(data["groundtruth_is_crowd"], tf.int32),
+        tf.cast(data['groundtruth_is_crowd'], tf.int32),
         self._max_num_instances, 0)
     labels = {
-        "source_id": data["source_id"],
-        "bbox": boxes,
-        "classes": classes,
-        "area": area,
-        "is_crowd": is_crowd,
-        "best_anchors": best_anchors,
-        "width": shape[1],
-        "height": shape[2],
-        "num_detections": tf.shape(data["groundtruth_classes"])[0],
+        'source_id': data['source_id'],
+        'bbox': boxes,
+        'classes': classes,
+        'area': area,
+        'is_crowd': is_crowd,
+        'best_anchors': best_anchors,
+        'width': shape[1],
+        'height': shape[2],
+        'num_detections': tf.shape(data['groundtruth_classes'])[0],
     }
     return image, labels
